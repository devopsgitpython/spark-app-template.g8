// ****** Application Properties ******

spark.master=local
spark.app.name="$name;format="Camel"$"

spark.driver.cores=1
spark.driver.memory=1g
spark.driver.memoryOverhead=384
spark.driver.maxResultSize=1g

spark.executor.cores=1
spark.executor.memory=1g
spark.executor.memoryOverhead=384

spark.local.dir=/tmp
spark.logConf=false
spark.submit.deployMode=client

//spark.files=
//spark.jars=

// ****** Shuffle Behavior ******
//spark.reducer.maxSizeInFlight=48m
//spark.reducer.maxReqsInFlight=Int.MaxValue
//spark.reducer.maxBlocksInFlightPerAddress=Int.MaxValue
//spark.maxRemoteBlockSizeFetchToMem=Long.MaxValue
//spark.shuffle.compress=true
//spark.shuffle.file.buffer=32k
//spark.shuffle.io.maxRetries=3
//spark.shuffle.io.numConnectionsPerPeer=1
//spark.shuffle.io.preferDirectBufs=true
//spark.shuffle.io.retryWait=5s
//spark.shuffle.service.enabled=false
//spark.shuffle.service.port=7337
//spark.shuffle.service.index.cache.size=100m
//spark.shuffle.maxChunksBeingTransferred=Long.MAX_VALUE
//spark.shuffle.sort.bypassMergeThreshold=200
//spark.shuffle.spill.compress=true
//spark.shuffle.accurateBlockThreshold="100 * 1024 * 1024"
//spark.shuffle.registration.timeout=5000
//spark.shuffle.registration.maxAttempts=3
//spark.io.encryption.enabled=false
//spark.io.encryption.keySizeBits=128
//spark.io.encryption.keygen.algorithm=HmacSHA1

// ****** Spark UI ******
//spark.eventLog.logBlockUpdates.enabled=false
//spark.eventLog.compress=false
//spark.eventLog.dir="file:///tmp/spark-events"
//spark.eventLog.enabled=false
//spark.eventLog.overwrite=false
//spark.eventLog.buffer.kb=100k
//spark.ui.enabled=true
//spark.ui.killEnabled=true
//spark.ui.port=4040
//spark.ui.retainedJobs=1000
//spark.ui.retainedStages=1000
//spark.ui.retainedTasks=100000
//spark.ui.reverseProxy=false
//spark.ui.reverseProxyUrl=
//spark.ui.showConsoleProgress=true
//spark.worker.ui.retainedExecutors=1000
//spark.worker.ui.retainedDrivers=1000
//spark.sql.ui.retainedExecutions=1000
//spark.streaming.ui.retainedBatches=1000
//spark.ui.retainedDeadExecutors=10

// ****** Compression and Serialization ******
//spark.broadcast.compress=true
//spark.io.compression.codec=lz4
//spark.io.compression.lz4.blockSize=32k
//spark.io.compression.snappy.blockSize=32k
//spark.io.compression.zstd.level=1
//spark.io.compression.zstd.bufferSize=32k
//spark.kryo.classesToRegister=
//spark.kryo.referenceTracking=true
//spark.kryo.registrationRequired=false
//spark.kryo.registrator=
//spark.kryo.unsafe=false
//spark.kryoserializer.buffer.max=64m
//spark.kryoserializer.buffer=64k
//spark.rdd.compress=false
//spark.serializer="org.apache.spark.serializer.JavaSerializer"
//spark.serializer.objectStreamReset=100

// ****** Memory Management ******
//spark.memory.fraction=0.6
//spark.memory.storageFraction=0.5
//spark.memory.offHeap.enabled=false
//spark.memory.offHeap.size=0
//spark.memory.useLegacyMode=false
//spark.shuffle.memoryFraction=0.2
//spark.storage.memoryFraction=0.6
//spark.storage.unrollFraction=0.2
//spark.storage.replication.proactive=false
//spark.cleaner.periodicGC.interval=30min
//spark.cleaner.referenceTracking=true
//spark.cleaner.referenceTracking.blocking=true
//spark.cleaner.referenceTracking.blocking.shuffle=false
//spark.cleaner.referenceTracking.cleanCheckpoints=false

// ****** Execution Behavior ******
//spark.broadcast.blockSize=4m
//spark.default.parallelism=
//spark.executor.heartbeatInterval=10s
//spark.files.fetchTimeout=60s
//spark.files.useFetchCache=true
//spark.files.overwrite=false
//spark.files.maxPartitionBytes=134217728
//spark.files.openCostInBytes=4194304
//spark.hadoop.cloneConf=false
//spark.hadoop.validateOutputSpecs=true
//spark.storage.memoryMapThreshold=2m
//spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=1

// ****** Networking ******
//spark.rpc.message.maxSize=128
//spark.blockManager.port=
//spark.driver.blockManager.port=
//spark.driver.bindAddress=
//spark.driver.host=
//spark.driver.port=
//spark.network.timeout=120s
//spark.port.maxRetries=16
//spark.rpc.numRetries=3
//spark.rpc.retry.wait=3s
//spark.rpc.askTimeout="spark.network.timeout"
//spark.rpc.lookupTimeout=120s

// ******Scheduling ******
//spark.cores.max=
//spark.locality.wait=3s
//spark.locality.wait.node=
//spark.locality.wait.process=
//spark.locality.wait.rack=
//spark.scheduler.maxRegisteredResourcesWaitingTime=30s
//spark.scheduler.minRegisteredResourcesRatio=0.8
//spark.scheduler.mode=FIFO
//spark.scheduler.revive.interval=1s
//spark.scheduler.listenerbus.eventqueue.capacity=10000
//spark.blacklist.enabled=false
//spark.blacklist.timeout=1h
//spark.blacklist.task.maxTaskAttemptsPerExecutor=1
//spark.blacklist.task.maxTaskAttemptsPerNode=2
//spark.blacklist.stage.maxFailedTasksPerExecutor=2
//spark.blacklist.stage.maxFailedExecutorsPerNode=2
//spark.blacklist.application.maxFailedTasksPerExecutor=2
//spark.blacklist.application.maxFailedExecutorsPerNode=2
//spark.blacklist.killBlacklistedExecutors=false
//spark.blacklist.application.fetchFailure.enabled=false
//spark.speculation=false
//spark.speculation.interval=100ms
//spark.speculation.multiplier=1.5
//spark.speculation.quantile=0.75
//spark.task.cpus=1
//spark.task.maxFailures=4
//spark.task.reaper.enabled=false
//spark.task.reaper.pollingInterval=10s
//spark.task.reaper.threadDump=true
//spark.task.reaper.killTimeout=4
//spark.stage.maxConsecutiveAttempts=4

// ****** Dynamic Allocation ******
//spark.dynamicAllocation.enabled=false
//spark.dynamicAllocation.executorIdleTimeout=60s
//spark.dynamicAllocation.cachedExecutorIdleTimeout=infinity
//spark.dynamicAllocation.initialExecutors="spark.dynamicAllocation.minExecutors"
//spark.dynamicAllocation.maxExecutors=infinity
//spark.dynamicAllocation.minExecutors=0
//spark.dynamicAllocation.schedulerBacklogTimeout=1s
//spark.dynamicAllocation.sustainedSchedulerBacklogTimeout="schedulerBacklogTimeout"

// ****** Security ******
//spark.acls.enable=false
//spark.admin.acls=Empty
//spark.admin.acls.groups=Empty
//spark.user.groups.mapping="org.apache.spark.security.ShellBasedGroupsMappingProvider"
//spark.authenticate=false
//spark.authenticate.secret=None
//spark.network.crypto.enabled=false
//spark.network.crypto.keyLength=128
//spark.network.crypto.keyFactoryAlgorithm="PBKDF2WithHmacSHA1"
//spark.network.crypto.saslFallback=true
//spark.network.crypto.config.*=None
//spark.authenticate.enableSaslEncryption=false
//spark.network.sasl.serverAlwaysEncrypt=false
//spark.core.connection.ack.wait.timeout="spark.network.timeout"
//spark.modify.acls=Empty
//spark.modify.acls.groups=Empty
//spark.ui.filters=None
//spark.ui.view.acls=Empty
//spark.ui.view.acls.groups=Empty

// ****** TLS / SSL ******
//spark.ssl.enabled=false
//spark.ssl.[namespace].port=None
//spark.ssl.enabledAlgorithms=Empty
//spark.ssl.keyPassword=
//spark.ssl.keyStore=
//spark.ssl.keyStorePassword=
//spark.ssl.keyStoreType=JKS
//spark.ssl.protocol=None
//spark.ssl.needClientAuth=false
//spark.ssl.trustStore=None
//spark.ssl.trustStorePassword=None
//spark.ssl.trustStoreType=JKS

// ****** Spark SQL ******
//spark.streaming.backpressure.enabled=false
//spark.streaming.backpressure.initialRate=
//spark.streaming.blockInterval=200ms
//spark.streaming.receiver.maxRate=
//spark.streaming.receiver.writeAheadLog.enable=false
//spark.streaming.unpersist=true
//spark.streaming.stopGracefullyOnShutdown=false
//spark.streaming.kafka.maxRatePerPartition=
//spark.streaming.kafka.maxRetries=1
//spark.streaming.ui.retainedBatches=1000
//spark.streaming.driver.writeAheadLog.closeFileAfterWrite=false
//spark.streaming.receiver.writeAheadLog.closeFileAfterWrite=false

// ****** SparkR ******
//spark.r.numRBackendThreads=2
//spark.r.command=Rscript
//spark.r.driver.command="spark.r.command"
//spark.r.shell.command=R
//spark.r.backendConnectionTimeout=6000
//spark.r.heartBeatInterval=100

// ****** GraphX ******
//spark.graphx.pregel.checkpointInterval=-1

// ****** Deploy ******
//spark.deploy.recoveryMode=NONE
//spark.deploy.zookeeper.url=None
//spark.deploy.zookeeper.dir=None